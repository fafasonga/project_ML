{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = scipy.io.loadmat('./Dataset/glass.mat')['X']\n",
    "data1_label = scipy.io.loadmat('./Dataset/glass.mat')['y'].astype(int)\n",
    "\n",
    "data2 = scipy.io.loadmat('./Dataset/mnist.mat')['X']\n",
    "data2_label = scipy.io.loadmat('./Dataset/mnist.mat')['y'].astype(int)\n",
    "\n",
    "data3 = scipy.io.loadmat('./Dataset/satimage-2.mat')['X']\n",
    "data3_label = scipy.io.loadmat('./Dataset/satimage-2.mat')['y'].astype(int)\n",
    "\n",
    "data4 = scipy.io.loadmat('./Dataset/speech.mat')['X']\n",
    "data4_label = scipy.io.loadmat('./Dataset/speech.mat')['y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data1=scaler.fit_transform(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948051948051948\n",
      "1.0\n",
      "0.993140243902439\n",
      "0.9054878048780488\n",
      "1.0\n",
      "0.9940119760479041\n",
      "0.922943722943723\n",
      "0.9489177489177489\n",
      "0.9047256097560976\n",
      "0.9956709956709957\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data1, data1_label, test_size=0.8)\n",
    "    model = Sequential()\n",
    "    # Add an input layer \n",
    "    model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    # Add one hidden layer \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # Add an output layer \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,epochs=50, batch_size=1, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    # print(confusion_matrix(y_test, y_pred))\n",
    "    print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[99.48051948051948,\n",
    "100.0,\n",
    "99.3140243902439,\n",
    "90.54878048780488,\n",
    "100.0,\n",
    "99.40119760479041,\n",
    "92.2943722943723,\n",
    "94.89177489177489,\n",
    "90.47256097560976,\n",
    "99.56709956709957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957,\n",
       " 0.9948051948051948,\n",
       " 1.0,\n",
       " 0.993140243902439,\n",
       " 0.9054878048780488,\n",
       " 1.0,\n",
       " 0.9940119760479041,\n",
       " 0.922943722943723,\n",
       " 0.9489177489177489,\n",
       " 0.9047256097560976,\n",
       " 0.9956709956709957]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'figure.constrained_layout.use'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-181ba6519428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_xticks, autorange, zorder, hold, data)\u001b[0m\n\u001b[1;32m   2823\u001b[0m             \u001b[0mwhiskerprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanage_xticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautorange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m             hold=None, data=None):\n\u001b[0;32m-> 2825\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2826\u001b[0m     \u001b[0;31m# Deprecated: allow callers to override the hold state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;31m# by passing hold=True|False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mgca\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \"\"\"\n\u001b[0;32m--> 984\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;31m# More ways of creating axes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgcf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfigManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m                                         \u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                                         \u001b[0mFigureClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFigureClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfigLabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[0;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mfig_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FigureClass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_figure_manager_given_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, figsize, dpi, facecolor, edgecolor, linewidth, frameon, subplotpars, tight_layout, constrained_layout)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layoutbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# set in set_constrained_layout_pads()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_constrained_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained_layout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mset_constrained_layout\u001b[0;34m(self, constrained)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constrained_layout_pads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hspace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconstrained\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mconstrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.constrained_layout.use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mst_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISREG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISFIFO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;31m# Return first candidate that is a file, or last candidate if none is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'figure.constrained_layout.use'"
     ]
    }
   ],
   "source": [
    "plt.boxplot(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data2=scaler.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1123710898501718, 0.9850402761795167]\n",
      "0.9956688961030495\n",
      "[0.12944075701693056, 0.9833963504849581]\n",
      "0.9933747926705603\n",
      "[0.10598689601972917, 0.9852046687489725]\n",
      "0.9944935909008846\n",
      "[0.1502272920817005, 0.9815880322209436]\n",
      "0.9940709724043438\n",
      "[0.129276356682584, 0.9809304619431202]\n",
      "0.9916410028005019\n",
      "[0.13343673976147302, 0.9825743876474773]\n",
      "0.9909861537684316\n",
      "[0.150285912536344, 0.981094854512576]\n",
      "0.9925990538602734\n",
      "[0.2310791459653874, 0.978300180841625]\n",
      "0.9912380472756182\n",
      "[0.1895677870657329, 0.9797797139569292]\n",
      "0.9930060494007363\n",
      "[0.11182277800244195, 0.9845470984711491]\n",
      "0.9875500429008323\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data2, data2_label, test_size=0.8)\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer \n",
    "    model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    # Add one hidden layer \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # Add an output layer \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,epochs=20, batch_size=1, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(score)\n",
    "    # print(confusion_matrix(y_test, y_pred))\n",
    "    print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data3=scaler.fit_transform(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-89e85accd6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2478\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data3, data3_label, test_size=0.8)\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer \n",
    "    model.add(Dense(20, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    # Add one hidden layer \n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    # Add an output layer \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,epochs=20, batch_size=1, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(score)\n",
    "    # print(confusion_matrix(y_test, y_pred))\n",
    "    roc_result = roc_auc_score(y_test, y_pred)\n",
    "    print(roc_result)\n",
    "    result.append(roc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data4=scaler.fit_transform(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data4, data4_label, test_size=0.7)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer \n",
    "    model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    # Add one hidden layer \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # Add an output layer \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,epochs=20, batch_size=1, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(score)\n",
    "    # print(confusion_matrix(y_test, y_pred))\n",
    "    roc_result = roc_auc_score(y_test, y_pred)\n",
    "    print(roc_result)\n",
    "    result.append(roc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
